from collections import Counter
import json
import math
import csv
import random
import re
from typing import Any, Sequence, List, Dict, Tuple


class Network:

    EPSILON = 1e-15

    # ================================================================
    #  ä¸€ã€Activation Functions
    # ================================================================
    class Activation:
        """å®šç¾©å¸¸è¦‹æ¿€å‹µå‡½æ•¸"""

        @staticmethod
        def linear(x: Sequence[float]) -> List[float]:
            return list(x)

        @staticmethod
        def linear_derivative(x: float) -> float:
            return 1

        @staticmethod
        def relu(x: Sequence[float]) -> List[float]:
            return [max(0, v) for v in x]

        @staticmethod
        def relu_derivative(x: float) -> float:
            return 1 if x > 0 else 0

        @staticmethod
        def sigmoid(x: Sequence[float]) -> List[float]:
            return [1 / (1 + math.exp(-v)) for v in x]

        @staticmethod
        def sigmoid_derivative(node_output: float) -> float:
            return node_output * (1 - node_output)

        @staticmethod
        def softmax(x: Sequence[float]) -> List[float]:
            max_x = max(x)
            exp_x = [math.exp(v - max_x) for v in x]
            total = sum(exp_x)
            return [v / total for v in exp_x]

    # ================================================================
    # äºŒã€Loss Functions
    # ================================================================
    class Loss:
        """å®šç¾©å¸¸è¦‹æå¤±å‡½æ•¸"""

        @staticmethod
        def mse(expects, outputs):
            """Mean Squared Error"""
            expects = Network._to_list(expects)
            outputs = Network._to_list(outputs)
            n = len(expects)
            return sum((e - o) ** 2 for e, o in zip(expects, outputs)) / n

        @staticmethod
        def mse_derivative(expect: float, output: float) -> float:
            return 2 * (output - expect)

        @staticmethod
        def binary_cross_entropy(expects, outputs):
            expects = Network._to_list(expects)
            outputs = Network._to_list(outputs)
            return -sum(
                (e * math.log(o)) + ((1 - e) * math.log(1 - o))
                for e, o in zip(expects, outputs)
            )

        @staticmethod
        def binary_cross_entropy_derivative(expect: float, output: float) -> float:
            return ((1 - expect) / (1 - output)) - (expect / output)

        @staticmethod
        def categorical_cross_entropy(expects, outputs):
            expects = Network._to_list(expects)
            outputs = Network._to_list(outputs)
            return -sum(e * math.log(o) for e, o in zip(expects, outputs))

    # ================================================================
    #  ä¸‰ã€ç¯€é»ï¼ˆNodeï¼‰
    # ================================================================
    class _Node:
        """ç¥ç¶“ç¶²è·¯ä¸­çš„å–®ä¸€ç¯€é»"""

        def __init__(
            self,
            pre_layer: "Network._Layer" = None,
            weights: Sequence[float] = None,
            node_type: str = "I",
            layer_idx: int = None,
            node_idx: int = None,
        ):
            self.id = self._generate_id(node_type, layer_idx, node_idx)
            self.pre_layer = pre_layer
            self.raw_output = None
            self.output = 1 if node_type == "B" else None  # è¨­å®š bias output
            self.weight_mapping = {}
            self.d_TL_d_raw = None
            self.d_activation_output = 0
            self.gradient_mapping = {}

            # è‹¥æœ‰å‰ä¸€å±¤èˆ‡æ¬Šé‡ï¼Œå»ºç«‹å°æ‡‰è¡¨
            weights = list(weights or [])
            if self.pre_layer and weights:
                if len(self.pre_layer.nodes) != len(weights):
                    raise ValueError(f"{self.id}: å‰ä¸€å±¤ç¯€é»æ•¸èˆ‡æ¬Šé‡æ•¸é‡ä¸ä¸€è‡´")
                self.weight_mapping = dict(zip(self.pre_layer.nodes, weights))

        # ------------------------------------------------------------
        @staticmethod
        def _generate_id(node_type, layer_idx, node_idx):
            """è‡ªå‹•ç”Ÿæˆç¯€é» ID"""
            if node_type == "I":
                return f"I{node_idx + 1}"
            if node_type == "H":
                return f"H{(layer_idx or 0) + 1}_{node_idx + 1}"
            if node_type == "B":
                return f"B{(0 if layer_idx == None else layer_idx + 1)}"
            if node_type == "O":
                return f"O{node_idx + 1}"
            return f"N{node_idx}"

    # ================================================================
    #  å››ã€å±¤ï¼ˆLayerï¼‰
    # ================================================================
    class _Layer:
        """ç¥ç¶“ç¶²è·¯ä¸­çš„ä¸€å±¤ï¼ˆinput, hidden, outputï¼‰"""

        def __init__(
            self, nodes: List["Network._Node"], layer_type: str, activation: str
        ):
            self.nodes = nodes
            self.type = layer_type
            self.activation = activation.lower()

        def __getitem__(self, idx):
            """æ”¯æ´ç´¢å¼•èˆ‡åˆ‡ç‰‡"""
            return self.nodes[idx]

        def __iter__(self):
            """æ”¯æ´è¿­ä»£"""
            return iter(self.nodes)

    # ================================================================
    #  äº”ã€ç¶²è·¯çµæ§‹ï¼ˆNetworkMapï¼‰
    # ================================================================
    class NetworkMap:
        """å„²å­˜æ•´é«”ç¶²è·¯æ¶æ§‹èˆ‡é¡¯ç¤º"""

        def __init__(self):
            self.inputs: Network._Layer = None
            self.hidden: List[Network._Layer] = []
            self.outputs: Network._Layer = None

        def show(self):
            """ä»¥æ–‡å­—æ–¹å¼é¡¯ç¤ºç¶²è·¯çµæ§‹ã€é€£ç·šèˆ‡ activation"""
            print("\n=== Network Structure ===")

            # è¼¸å…¥å±¤
            print("\n[Input Layer] (activation: {})".format(self.inputs.activation))
            for n in self.inputs:
                print(f"  {n.id}")

            # éš±è—å±¤
            for idx, layer in enumerate(self.hidden, start=1):
                print(f"\n[Hidden Layer {idx}] (activation: {layer.activation})")
                for node in layer.nodes:
                    if node.id.startswith("B"):
                        continue
                    for pre_node, w in node.weight_mapping.items():
                        print(f"  {pre_node.id:<6} --{w:>6.2f}--> {node.id}")

            # è¼¸å‡ºå±¤
            print(f"\n[Output Layer] (activation: {self.outputs.activation})")
            for node in self.outputs.nodes:
                for pre_node, w in node.weight_mapping.items():
                    print(f"  {pre_node.id:<6} --{w:>6.2f}--> {node.id}")

            print("\n===========================\n")

        @property
        def layers(self):
            return [self.inputs] + self.hidden + [self.outputs]

        def __getitem__(self, idx):
            """æ”¯æ´ç´¢å¼•èˆ‡åˆ‡ç‰‡"""
            return self.layers[idx]

        def __iter__(self):
            """æ”¯æ´è¿­ä»£"""
            return iter(self.layers)

    # ================================================================
    #  å…­ã€ç¥ç¶“ç¶²è·¯ä¸»é«”ï¼ˆNetworkï¼‰
    # ================================================================
    def __init__(self, network_setting_json: str):
        config: dict = json.loads(network_setting_json)
        self.map = Network.NetworkMap()

        # å»ºç«‹ input nodes
        input_setting = config.get("input", {})
        self.map.inputs = self._build_layer(
            input_setting, pre_layer=None, layer_type="I"
        )

        pre_layer = self.map.inputs

        # å»ºç«‹ hidden layers
        for layer_idx, layer_cfg in enumerate(config.get("layer", [])):
            layer = self._build_layer(
                layer_cfg, pre_layer, layer_type="H", layer_idx=layer_idx
            )
            self.map.hidden.append(layer)
            pre_layer = layer

        # å»ºç«‹ output nodes
        output_cfg = config.get("output", {})
        self.map.outputs = self._build_layer(output_cfg, pre_layer, layer_type="O")

    # ------------------------------------------------------------
    def _build_layer(
        self,
        cfg: dict,
        pre_layer: "Network._Layer",
        layer_type: str,
        layer_idx: int = None,
    ) -> "Network._Layer":
        """å»ºç«‹ä¸€å±¤ï¼ˆinput / hidden / outputï¼‰"""

        activation = cfg.get("activation", "linear").lower()
        node_count = cfg.get("nodes", 0)

        # input layer
        if layer_type == "I":
            nodes = [
                Network._Node(node_type="I", node_idx=i) for i in range(node_count)
            ]
            nodes.append(Network._Node(node_type="B"))  # bias node
            return Network._Layer(nodes, layer_type="I", activation=activation)

        # hidden/output layer
        weights_matrix = cfg.get("weights", [])
        bias_weights = cfg.get("bias_weights", [])
        nodes = []

        for node_idx in range(node_count):
            weights = list(weights_matrix[node_idx]) + [bias_weights[node_idx]]
            node = Network._Node(
                pre_layer=pre_layer,
                weights=weights,
                node_type=layer_type,
                layer_idx=layer_idx,
                node_idx=node_idx,
            )
            nodes.append(node)

        # Hidden å±¤éœ€è¦ bias node
        if layer_type != "O":
            nodes.append(
                Network._Node(pre_layer=pre_layer, node_type="B", layer_idx=layer_idx)
            )
        return Network._Layer(nodes, layer_type=layer_type, activation=activation)

    # ------------------------------------------------------------
    def forward(self, input_values: Sequence[float]) -> List[float]:
        """åŸ·è¡Œå‰å‘å‚³éï¼ˆForward propagationï¼‰"""

        input_values = Network._to_list(input_values)
        if len(input_values) != len(self.map.inputs.nodes) - 1:  # å¿½ç•¥ bias
            raise ValueError("è¼¸å…¥æ•¸é‡èˆ‡ input node æ•¸é‡ä¸ä¸€è‡´")

        for val, node in zip(input_values, self.map.inputs[:-1]):
            node.output = val

        for layer, pre_layer in zip(self.map[1:], self.map[:-1]):
            # raw_output è¨ˆç®—
            nodes_raw_output_list = []
            for node in layer:
                if node.id.startswith("B"):
                    node.raw_output = 1
                    continue
                raw_output = sum(
                    pre_node.output * node.weight_mapping[pre_node]
                    for pre_node in pre_layer
                )
                node.raw_output = raw_output
                nodes_raw_output_list.append(raw_output)

            # activated_output è¨ˆç®—
            activation_func = getattr(
                Network.Activation,
                layer.activation,
                Network.Activation.linear,
            )
            activated_outputs = activation_func(nodes_raw_output_list)
            if len(layer.nodes) != len(activated_outputs):
                activated_outputs.append(1.0)  # bias output
            for node, output in zip(layer.nodes, activated_outputs):
                node.output = output

        return [node.output for node in self.map.outputs]

    # ------------------------------------------------------------
    def set_output_gradients(
        self,
        output_values: Sequence[float],
        expect_values: Sequence[float],
        loss_func: str,
    ):
        """æ‰‹å‹•è¨­å®šoutputs d_TL_d_raw"""
        output_values = Network._to_list(output_values)
        expect_values = Network._to_list(expect_values)
        if len(output_values) != len(expect_values):
            raise ValueError("outputs æ•¸é‡èˆ‡ expects æ•¸é‡ä¸ä¸€è‡´")
        loss_func = loss_func.strip().lower()
        d_loss_func = getattr(Network.Loss, loss_func + "_derivative", None)
        output_layer_activation = self.map.outputs.activation
        d_output_layer_activation_func = getattr(
            Network.Activation,
            output_layer_activation + "_derivative",
            None,
        )
        for o, e, output_node in zip(output_values, expect_values, self.map.outputs):
            act_input = (
                output_node.output
                if output_layer_activation == "sigmoid"
                else output_node.raw_output
            )

            d_TL_d_raw = (
                d_loss_func(e, o) / (len(output_values) if loss_func == "mse" else 1)
            ) * d_output_layer_activation_func(act_input)
            output_node.d_TL_d_raw = d_TL_d_raw

            output_node.gradient_mapping = {
                pre_node: d_TL_d_raw * pre_node.output
                for pre_node in self.map.layers[-2]
            }

    # ------------------------------------------------------------
    def backward(self):
        """è¨ˆç®—æ¢¯åº¦"""

        rev_list = self.map.layers[::-1]
        for next_layer, layer, pre_layer in zip(rev_list, rev_list[1:], rev_list[2:]):

            d_activation = getattr(
                Network.Activation,
                layer.activation + "_derivative",
                Network.Activation.linear_derivative,
            )
            node_d_act_input_type = (
                "output" if layer.activation == "sigmoid" else "raw_output"
            )
            for node in layer:
                if node.id.startswith("B"):
                    continue
                d_activation_output = d_activation(getattr(node, node_d_act_input_type))
                node.d_activation_output = d_activation_output
                d_TL_d_raw = d_activation_output * (
                    sum(
                        next_layer_node.d_TL_d_raw
                        * next_layer_node.weight_mapping[node]
                        for next_layer_node in next_layer
                        if not next_layer_node.id.startswith("B")
                    )
                )
                node.d_TL_d_raw = d_TL_d_raw
                node.gradient_mapping = {
                    pre_node: d_TL_d_raw * pre_node.output for pre_node in pre_layer
                }

    # ------------------------------------------------------------
    def show_gradient(self) -> str:
        parts = ["====NetWork Current Gradient===="]
        for li, layer in enumerate(self.map[1:]):  # skip input layer
            # åˆ¤æ–·å±¤åç¨±
            if layer.type == "O":
                layer_name = "Output Layer"
            else:
                layer_name = f"Hidden Layer {li+1}"

            parts.append(f"{layer_name}:")
            for node in layer:
                # å‰ä¸€å±¤åç¨±
                pre_layer_name = (
                    "Input Layer"
                    if node.pre_layer.type == "I"
                    else f"Hidden Layer {self.map.hidden.index(node.pre_layer)+1}"
                )
                grad_dict = {
                    pre_node.id: grad
                    for pre_node, grad in node.gradient_mapping.items()
                }
                parts.append(f"{node.id} <---> {pre_layer_name} {grad_dict}")
        return "\n".join(parts)

    # ------------------------------------------------------------
    def zero_grad(self, learning_rate: float):
        """æ›´æ–°æ¬Šé‡ä¸¦æ­¸é›¶æ¢¯åº¦ï¼Œå›å‚³æ›´æ–°å¾Œçš„æ¢¯åº¦å­—ä¸²"""
        for layer in self.map[1:]:
            for node in layer:
                for pre_node in node.weight_mapping:
                    node.weight_mapping[pre_node] -= (
                        node.gradient_mapping[pre_node] * learning_rate
                    )
                    node.gradient_mapping[pre_node] = 0

    # ------------------------------------------------------------
    def show_weights(self):
        """å»ºç«‹æ–‡å­—è¼¸å‡º"""
        parts = ["====NetWork Current weight===="]
        for li, layer in enumerate(self.map[1:]):
            if li == 0:
                layer_name = "Input Layer"
            elif li <= len(self.map.hidden):
                layer_name = f"Hidden Layer {li}"
            else:
                layer_name = "Output Layer"

            parts.append(f"{layer_name}:")

            for node in layer:
                if node.id.startswith("B"):
                    continue
                weight_dict = {
                    pre_node.id: w for pre_node, w in node.weight_mapping.items()
                }
                parts.append(f"  ---> {node.id} {weight_dict}")

        print("\n".join(parts))

    def generate_random_weight(self, start_num, end_num):
        for layer in self.map.layers[-1:0:-1]:
            for node in layer:
                if node.id.startswith("B"):
                    continue
                for key in node.weight_mapping:
                    node.weight_mapping[key] = random.uniform(start_num, end_num)

    @staticmethod
    def _to_list(x):
        if isinstance(x, tuple):
            return list(x)
        return x if isinstance(x, list) else [x]

    @staticmethod
    def build_model_json(
        input_nodes,
        hidden_nodes_list,
        output_nodes,
        hidden_activation="sigmoid",
        output_activation="linear",
        input_activation="linear",
    ):

        def rand():
            return random.uniform(-0.1, 0.1)

        def weight_matrix(rows, cols):
            return [[rand() for _ in range(cols)] for _ in range(rows)]

        def bias_vector(size):
            return [rand() for _ in range(size)]

        model = {}

        # input
        model["input"] = {"nodes": input_nodes, "activation": input_activation}

        # hidden layers
        layers = []
        prev_nodes = input_nodes
        for h_nodes in hidden_nodes_list:
            layer = {
                "nodes": h_nodes,
                "activation": hidden_activation,
                "weights": weight_matrix(h_nodes, prev_nodes),
                "bias_weights": bias_vector(h_nodes),
            }
            layers.append(layer)
            prev_nodes = h_nodes

        model["layer"] = layers

        # output layer
        model["output"] = {
            "nodes": output_nodes,
            "activation": output_activation,
            "weights": weight_matrix(output_nodes, prev_nodes),
            "bias_weights": bias_vector(output_nodes),
        }

        return model


# =====================================================
# åŸ·è¡Œ
# =====================================================
# ==========================================
# I. å…¨åŸŸå…±ç”¨è¨­å®šå’Œè¼”åŠ©å‡½æ•¸
# ==========================================
TEST_SIZE_RATIO = 0.2
RANDOM_SEED = 42


# --- CSV è®€å– ---
def read_csv_to_dict(path: str) -> List[Dict[str, str]]:
    data = []
    try:
        with open(path, mode="r", encoding="utf-8-sig") as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                data.append(dict(row))
    except FileNotFoundError:
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ {path}")
        return []
    return data


# --- çµ±è¨ˆè¨ˆç®— ---
def calculate_mean(data_list: list):
    clean_list = [x for x in data_list if x is not None]
    return sum(clean_list) / len(clean_list) if clean_list else 0.0


def calculate_std(data_list: list, mean: float):
    clean_list = [x for x in data_list if x is not None]
    if len(clean_list) <= 1:
        return 1.0
    variance = sum([(x - mean) ** 2 for x in clean_list]) / (len(clean_list) - 1)
    return math.sqrt(variance)


def calculate_median(data_list: list):
    clean_list = [x for x in data_list if x is not None]
    if not clean_list:
        return 0.0
    clean_list.sort()
    N = len(clean_list)
    return (
        (clean_list[N // 2 - 1] + clean_list[N // 2]) / 2
        if N % 2 == 0
        else clean_list[N // 2]
    )


def calculate_stats(values):
    # Task 1 ä¸­ä½¿ç”¨çš„ calculate_stats (ç”¨æ–¼è¨ˆç®— Mean å’Œ Std)
    if not values:
        return 0, 1
    mean = sum(values) / len(values)
    variance = sum((x - mean) ** 2 for x in values) / len(values)
    std = math.sqrt(variance)
    return mean, std


# --- æ•¸æ“šæ‹†åˆ† ---
def train_test_split_pure_python(
    features: List[List[float]], labels: List[Any], test_ratio: float, seed: int = 42
) -> Tuple[List[List[float]], List[List[float]], List[Any], List[Any]]:

    random.seed(seed)
    data_size = len(features)

    if data_size != len(labels):
        raise ValueError("ç‰¹å¾µçŸ©é™£å’Œæ¨™ç±¤çš„è¡Œæ•¸å¿…é ˆä¸€è‡´ã€‚")

    test_size = int(data_size * test_ratio)
    indices = list(range(data_size))
    random.shuffle(indices)

    test_indices = indices[:test_size]
    train_indices = indices[test_size:]

    # é€™è£¡çš„ labels æ˜¯ Any é¡å‹ï¼Œå› ç‚º Task 1 çš„æ¨™ç±¤æ˜¯ [float]ï¼ŒTask 2 æ˜¯ int
    X_train = [features[i] for i in train_indices]
    X_test = [features[i] for i in test_indices]
    Y_train = [labels[i] for i in train_indices]
    Y_test = [labels[i] for i in test_indices]

    return X_train, X_test, Y_train, Y_test


# ==========================================
# II. Task 1: Gender-Height-Weight (è¿´æ­¸)
# ==========================================

GENDER_FILE = "gender-height-weight.csv"


def preprocess_and_standardize_gender_data(
    raw_data_str: List[Dict[str, str]],
) -> Tuple[List[Dict[str, float]], List[Dict[str, float]], float, float, float, float]:
    """
    è™•ç† Task 1 çš„æ•¸æ“šï¼Œä½†ä¸åœ¨é€™è£¡æ‹†åˆ†å’Œè½‰æ›ç‚ºæœ€çµ‚çŸ©é™£ï¼Œè€Œæ˜¯è¿”å›è™•ç†å¾Œçš„å­—å…¸åˆ—è¡¨å’Œçµ±è¨ˆåƒæ•¸ã€‚
    """
    processed_data = []

    for row in raw_data_str:
        try:
            # A. è®€å–è³‡æ–™ä¸¦è½‰æ›å‹åˆ¥ (Gender: Male=0, Female=1)
            item = {
                "Gender": 0 if row["Gender"] == "Male" else 1,
                "Height": float(row["Height"]),
                "Weight": float(row["Weight"]),
            }
            processed_data.append(item)
        except ValueError:
            continue

    # B. åˆ‡åˆ†è¨“ç·´é›†èˆ‡æª¢æŸ¥é›†
    random.seed(RANDOM_SEED)
    random.shuffle(processed_data)
    split_idx = int(len(processed_data) * 0.8)
    train_set = processed_data[:split_idx]
    test_set = processed_data[split_idx:]

    # C. è¨ˆç®—çµ±è¨ˆæ•¸æ“š (åªä½¿ç”¨è¨“ç·´é›†)
    train_heights = [d["Height"] for d in train_set]
    train_weights = [d["Weight"] for d in train_set]
    h_mean, h_std = calculate_stats(train_heights)
    w_mean, w_std = calculate_stats(train_weights)

    return train_set, test_set, h_mean, h_std, w_mean, w_std


# ==========================================
# III. Task 2: Titanic (åˆ†é¡)
# ==========================================

TITANIC_FILE = "titanic.csv"
REGEX = {
    "WHOLE_STRING": r"^(.+)$",
    "NAME_TITLE": r" ([A-Za-z]+)\.",
    "FIRST_LETTER": r"([A-Z])",
}


# --- ç¨ç†±ç·¨ç¢¼  ---
def generate_minimal_one_hot_encoding(
    row_data: Dict[str, Any], column_name: str, regex_pattern: str, create_None: bool = False
) -> Dict[str, int]:
    one_hot_item = {}
    if column_name not in row_data:
        return one_hot_item
    cell_value = row_data[column_name]
    cell_value = str(cell_value) if cell_value is not None else ""
    match = re.search(regex_pattern, cell_value)

    if match:
        extracted_segment = match.group(1)
        one_hot_item[f"{column_name}_{extracted_segment}"] = 1
    elif create_None:
        one_hot_item[f"{column_name}_None"] = 1
    return one_hot_item


def preprocess_and_standardize_titanic_data(
    raw_data: List[Dict[str, Any]],
) -> Tuple[List[List[float]], List[int], int]:

    # --- Stage 1A: åˆ†é›¢ç›®æ¨™æ¨™ç±¤ (Y) å’Œåˆå§‹åŒ–ç‰¹å¾µåˆ—è¡¨ (X) ---
    target_labels = []
    data_for_processing = []

    for row in raw_data:
        try:
            survived_value = row.pop("Survived")
            target_labels.append(int(survived_value))
            data_for_processing.append(row)
        except (KeyError, ValueError):
            data_for_processing.append(row)
            continue

    # =================================================================
    # ğŸ”¥ æ–°å¢æ­¥é©Ÿï¼šè‡ªå‹•çµ±è¨ˆç¨±è¬‚é »ç‡ (è®“æ•¸æ“šæ±ºå®šèª°æ˜¯ç¨€æœ‰)
    # =================================================================
    all_raw_titles = []
    for row in data_for_processing:
        name_str = str(row.get("Name", ""))
        match = re.search(REGEX["NAME_TITLE"], name_str)
        if match:
            all_raw_titles.append(match.group(1))
    
    # è¨ˆç®—æ¯å€‹ç¨±è¬‚å‡ºç¾çš„æ¬¡æ•¸
    title_counts = Counter(all_raw_titles)
    
    # è¨­å®šé–€æª»ï¼šå‡ºç¾å°‘æ–¼ 10 æ¬¡çš„éƒ½ç•¶ä½œç¨€æœ‰
    MIN_FREQUENCY = 10 
    
    # å»ºç«‹ã€Œå¸¸è¦‹ç¨±è¬‚ã€çš„ç™½åå–® (Set æŸ¥è©¢é€Ÿåº¦å¿«)
    # ä¾‹å¦‚ï¼š{'Mr', 'Miss', 'Mrs', 'Master'}
    common_titles = {title for title, count in title_counts.items() if count >= MIN_FREQUENCY}
    
    print(f"[ç³»çµ±è‡ªå‹•åµæ¸¬] å¸¸è¦‹ç¨±è¬‚ (å‡ºç¾æ¬¡æ•¸>={MIN_FREQUENCY}): {common_titles}")
    # ================================================================= 

    processed_data = []
    all_keys = {
        "Pclass": set(),
        "Name": set(),
        "Cabin": set(),
        "Embarked": set(),
    }

    # --- Stage 1B: ç‰¹å¾µæå–èˆ‡ç¼ºå¤±æ¨™è¨» (X) ---
    for row in data_for_processing:
        item = {}

        # è™•ç†é¡åˆ¥ç‰¹å¾µ
        pclass_dict = generate_minimal_one_hot_encoding(
            row, "Pclass", REGEX["WHOLE_STRING"]
        )
        item.update(pclass_dict)
        all_keys["Pclass"].update(pclass_dict.keys())
        
        # -------------------------------------------------------------
        # ğŸ”¥ ä¿®æ”¹ Name è™•ç†é‚è¼¯ï¼šå‹•æ…‹æ˜ å°„
        # -------------------------------------------------------------
        name_str = str(row.get("Name", ""))
        match = re.search(REGEX["NAME_TITLE"], name_str)
        
        clean_title = "Rare" # é è¨­ç‚ºç¨€æœ‰
        if match:
            raw_title = match.group(1)
            # æª¢æŸ¥é€™å€‹ç¨±è¬‚æ˜¯å¦åœ¨æˆ‘å€‘å‰›ç®—å‡ºä¾†çš„ç™½åå–®å…§
            if raw_title in common_titles:
                clean_title = raw_title
            else:
                clean_title = "Rare"
        
        # æ‰‹å‹•å»ºç«‹ One-Hot éµå€¼
        title_key = f"Name_{clean_title}"
        item[title_key] = 1
        all_keys["Name"].add(title_key)
        
        
        item["Sex"] = 1 if row["Sex"] == "male" else 0

        # è™•ç†Age (ç¼ºå¤±æ¨™è¨»)
        age_value = row.get("Age")
        is_age_missing = age_value is None or age_value == ""
        item["Age_Missing"] = 1 if is_age_missing else 0
        item["Age"] = (
            float(age_value) if not is_age_missing and age_value is not None else None
        )

        # è™•ç†SibSp å’Œ Parch (è½‰ç‚º float)
        item["SibSp"] = float(row["SibSp"])
        item["Parch"] = float(row["Parch"])

        # è™•ç†Fare (ç¼ºå¤±æ¨™è¨»)
        fare_value = row.get("Fare")
        is_fare_missing = fare_value is None or fare_value == ""
        item["Fare_Missing"] = 1 if is_fare_missing else 0
        item["Fare"] = (
            float(fare_value)
            if not is_fare_missing and fare_value is not None
            else None
        )

        # è™•ç†Cabin å’Œ Embarked
        cabin_dict = generate_minimal_one_hot_encoding(
            row, "Cabin", REGEX["FIRST_LETTER"], True
        )
        item.update(cabin_dict)
        all_keys["Cabin"].update(cabin_dict.keys())
        embarked_dict = generate_minimal_one_hot_encoding(
            row, "Embarked", REGEX["WHOLE_STRING"], True
        )
        item.update(embarked_dict)
        all_keys["Embarked"].update(embarked_dict.keys())

        processed_data.append(item)

    # --- Stage 2A/2B: å¡«è£œã€å°æ•¸è½‰æ› (Fare) ä¸¦æ”¶é›†æ•¸æ“š ---
    all_ages = [row.get("Age") for row in processed_data]
    all_fares = [row.get("Fare") for row in processed_data]
    MEDIAN_AGE = calculate_median(all_ages)
    MEDIAN_FARE = calculate_median(all_fares)
    temp_age_list, temp_fare_list, temp_sibsp_list, temp_parch_list = [], [], [], []

    for row in processed_data:
        if row["Age"] is None:
            row["Age"] = MEDIAN_AGE
        temp_age_list.append(row["Age"])

        if row["Fare"] is None:
            row["Fare"] = MEDIAN_FARE
        row["Fare"] = math.log(row["Fare"] + 1)
        temp_fare_list.append(row["Fare"])

        temp_sibsp_list.append(row["SibSp"])
        temp_parch_list.append(row["Parch"])

    # --- Stage 2C/2D: è¨ˆç®—å’ŒåŸ·è¡Œ Z-Score æ¨™æº–åŒ– ---
    MEAN_AGE = calculate_mean(temp_age_list)
    STD_AGE = calculate_std(temp_age_list, MEAN_AGE)
    MEAN_FARE = calculate_mean(temp_fare_list)
    STD_FARE = calculate_std(temp_fare_list, MEAN_FARE)
    MEAN_SIBSP = calculate_mean(temp_sibsp_list)
    STD_SIBSP = calculate_std(temp_sibsp_list, MEAN_SIBSP)
    MEAN_PARCH = calculate_mean(temp_parch_list)
    STD_PARCH = calculate_std(temp_parch_list, MEAN_PARCH)

    for row in processed_data:
        row["Age"] = (row["Age"] - MEAN_AGE) / STD_AGE
        row["Fare"] = (row["Fare"] - MEAN_FARE) / STD_FARE
        row["SibSp"] = (row["SibSp"] - MEAN_SIBSP) / STD_SIBSP
        row["Parch"] = (row["Parch"] - MEAN_PARCH) / STD_PARCH

    # --- Stage 2E: çµ±ä¸€ç‰¹å¾µç©ºé–“ä¸¦è½‰æ›ç‚ºçŸ©é™£ (final_feature_matrix) ---
    SCALAR_KEYS = [
        "Sex",
        "Age",
        "Age_Missing",
        "SibSp",
        "Parch",
        "Fare",
        "Fare_Missing",
    ]
    ONE_HOT_KEYS = [
        k
        for key_type in ["Pclass", "Name", "Cabin", "Embarked"]
        for k in sorted(list(all_keys[key_type]))
    ]
    FINAL_FEATURE_MAP = SCALAR_KEYS + ONE_HOT_KEYS

    print("FINAL_FEATURE_MAP = " ,FINAL_FEATURE_MAP)

    final_feature_matrix = []
    for row_dict in processed_data:
        feature_vector = [float(row_dict.get(key, 0.0)) for key in FINAL_FEATURE_MAP]
        final_feature_matrix.append(feature_vector)

    return final_feature_matrix, target_labels, len(FINAL_FEATURE_MAP)

def build_titanic_model_config(input_nodes: int):
    """
    [Task 2 è¼”åŠ©] ç”Ÿæˆ Titanic ç¥ç¶“ç¶²è·¯æ¨¡å‹çš„ JSON è¨­å®šçµæ§‹
    """
    # éš±è—å±¤çµæ§‹ï¼šå˜—è©¦å…©å±¤ï¼Œç¯€é»æ•¸ä»‹æ–¼è¼¸å…¥(ç´„12-15)èˆ‡è¼¸å‡º(1)ä¹‹é–“
    hidden_nodes_list = [20, 10, 4] 
    
    # è¼¸å‡ºå±¤ï¼š1 å€‹ç¯€é» (ç”Ÿå­˜æ©Ÿç‡)ï¼Œæ¿€æ´»å‡½æ•¸ä¸€å®šè¦ç”¨ Sigmoid (å°‡è¼¸å‡ºå£“åœ¨ 0~1)
    output_nodes = 1
    output_activation = "sigmoid" 
    hidden_activation = "sigmoid" 

    # å‘¼å« Network é¡åˆ¥çš„éœæ…‹æ–¹æ³•
    model_config = Network.build_model_json(
        input_nodes=input_nodes,
        hidden_nodes_list=hidden_nodes_list,
        output_nodes=output_nodes,
        hidden_activation=hidden_activation,
        output_activation=output_activation
    )
    return model_config


def calculate_accuracy(predictions: List[List[float]], targets: List[int]) -> float:
    """
    [Task 2 è¼”åŠ©] è¨ˆç®—åˆ†é¡æº–ç¢ºç‡ (é–¾å€¼ 0.5)
    """
    correct = 0
    total = len(targets)
    if total == 0: return 0.0
    
    for pred_vector, target in zip(predictions, targets):
        # pred_vector[0] æ˜¯æ¨¡å‹è¼¸å‡ºçš„æ©Ÿç‡ (ä¾‹å¦‚ 0.72)
        # å¦‚æœ >= 0.5 é æ¸¬ç‚º 1 (å­˜æ´»)ï¼Œå¦å‰‡ç‚º 0 (æ­»äº¡)
        pred_label = 1 if pred_vector[0] >= 0.5 else 0
        
        if pred_label == target:
            correct += 1
            
    return correct / total

# ==========================================
# IV. ä¸»åŸ·è¡Œæµç¨‹
# ==========================================


def run_task1_training():
    """åŸ·è¡Œ Gender-Height-Weight è¿´æ­¸è¨“ç·´å’Œè©•ä¼°"""

    raw_data_str = read_csv_to_dict(GENDER_FILE)

    # åŸ·è¡Œæ•¸æ“šè™•ç†ï¼Œå¾—åˆ°è¨“ç·´/æ¸¬è©¦é›†å’Œçµ±è¨ˆåƒæ•¸
    train_set, test_set, h_mean, h_std, w_mean, w_std = (
        preprocess_and_standardize_gender_data(raw_data_str)
    )

    print(f"è³‡æ–™è¼‰å…¥å®Œæˆ: ç¸½ç­†æ•¸ {len(train_set) + len(test_set)}")
    print(f"è¨“ç·´é›†: {len(train_set)}, æª¢æŸ¥é›†: {len(test_set)}")
    print(f"çµ±è¨ˆåƒæ•¸ (Train): èº«é«˜å‡å€¼={h_mean:.2f}, é«”é‡å‡å€¼={w_mean:.2f}")

    # æº–å‚™ç¶²è·¯è¼¸å…¥è³‡æ–™ (æ¨™æº–åŒ–)
    def create_dataset(source_data):
        inputs = []
        expects = []
        for row in source_data:
            h_norm = (row["Height"] - h_mean) / h_std
            inputs.append([row["Gender"], h_norm])
            w_norm = (row["Weight"] - w_mean) / w_std
            expects.append([w_norm])
        return inputs, expects

    train_inputs, train_expects = create_dataset(train_set)
    test_inputs, test_expects = create_dataset(test_set)

    # --- E. åˆå§‹åŒ–ç¶²è·¯ ---
    model_2h_json = """
    {
    "input": {
        "nodes": 2,
        "activation": "linear"
    },
    "layer": [
        {
        "nodes": 2,
        "activation": "sigmoid",
        "weights": [[0.1, -0.1], [-0.1, 0.1]],
        "bias_weights": [0.1, -0.1]
        }
    ],
    "output": {
        "nodes": 1,
        "activation": "linear",
        "weights": [[0.1, -0.1]],
        "bias_weights": [0.1]
    }
    }
    """

    # å‡è¨­é€™æ˜¯ä½ çš„ Network åˆå§‹åŒ–æ–¹å¼
    net = Network(model_2h_json)

    # æ³¨æ„ï¼šä½ åŸæœ¬ JSON è£¡æœ‰å¯«æ­» weightsï¼Œä½†é€™è£¡å‘¼å« random_weight æœƒæŠŠ JSON çš„è¦†è“‹æ‰
    # é€™é€šå¸¸æ˜¯æ­£ç¢ºçš„ï¼Œå› ç‚ºè¨“ç·´å‰æˆ‘å€‘å¸Œæœ›éš¨æ©Ÿåˆå§‹åŒ–
    net.generate_random_weight(-0.1, 0.1)
    print("åˆå§‹æ¬Šé‡:")
    net.show_weights()

    # è¨­å®š Loss Function (å›æ­¸å•é¡Œä½¿ç”¨ MSE)
    loss_func = Network.Loss.mse
    loss_derivative = Network.Loss.mse_derivative

    # --- F. è¨“ç·´è¿´åœˆ (Training Loop) ---
    learning_rate = 0.05
    epochs = 100  # è¨“ç·´å¹¾è¼ª

    print("\né–‹å§‹è¨“ç·´...")
    for epoch in range(epochs):

        if epoch == 40:
            learning_rate = learning_rate / 10
            print(f"\n[è¨Šæ¯] Epoch {epoch}: å­¸ç¿’ç‡å·²èª¿æ•´ç‚º {learning_rate}\n")

        if epoch == 70:
            learning_rate = learning_rate / 10
            print(f"\n[è¨Šæ¯] Epoch {epoch}: å­¸ç¿’ç‡å·²èª¿æ•´ç‚º {learning_rate}\n")

        total_loss = 0

        for x, y in zip(train_inputs, train_expects):
            # 1. Forward
            output = net.forward(x)

            # 2. Calculate Loss (åƒ…ä¾›è§€å¯Ÿ)
            loss = loss_func(y, output)
            total_loss += loss

            # 3. Backward (é€™éƒ¨åˆ†ä¾è³´ä½  Network é¡åˆ¥çš„å¯¦ä½œï¼Œå‡è¨­æœ‰é€™å€‹æ–¹æ³•)
            net.set_output_gradients(output, y, "mse")
            net.backward()
            net.zero_grad(learning_rate)

        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Avg Loss = {total_loss / len(train_inputs):.6f}")

    # --- G. æœ€çµ‚æ¸¬è©¦è©•ä¼° (Evaluation) ---
    print("\n========================================")
    print("é–‹å§‹æœ€çµ‚æ¸¬è©¦ (è©•ä¼°æ•´å€‹æª¢æŸ¥é›†)...")
    print("========================================")

    total_test_loss = 0
    total_abs_error_lbs = 0  # ç”¨ä¾†ç´¯ç©çµ•å°èª¤å·® (ç£…)

    # èµ°è¨ªæ¯ä¸€ç­†æ¸¬è©¦è³‡æ–™
    for x, y in zip(test_inputs, test_expects):

        # 1. Forward (åªåšå‰å‘å‚³æ’­ï¼Œä¸åšåå‘å‚³æ’­!)
        output = net.forward(x)

        # 2. å–å¾—é æ¸¬å€¼èˆ‡çœŸå¯¦å€¼ (Z-score)
        pred_z = output[0]
        real_z = y[0]

        # 3. è¨ˆç®— MSE (æ¨™æº–åŒ–ç©ºé–“çš„ Loss) - ç”¨ä¾†è·Ÿè¨“ç·´æ™‚çš„ Loss æ¯”è¼ƒ
        loss = (pred_z - real_z) ** 2
        total_test_loss += loss

        # 4. é‚„åŸæˆçœŸå¯¦å–®ä½ (ç£…)
        pred_lbs = (pred_z * w_std) + w_mean
        real_lbs = (real_z * w_std) + w_mean

        # 5. è¨ˆç®—çµ•å°èª¤å·® (MAE) - é€™æ˜¯çµ¦äººé¡çœ‹çš„æŒ‡æ¨™
        # æˆ‘å€‘æƒ³çŸ¥é“å¹³å‡é æ¸¬å·®å¹¾ç£…ï¼Œè€Œä¸æ˜¯å·®å¹¾ç£…çš„å¹³æ–¹
        total_abs_error_lbs += abs(pred_lbs - real_lbs)

    # --- è¨ˆç®—å¹³å‡å€¼ ---
    avg_test_mse = total_test_loss / len(test_inputs)
    avg_mae_lbs = total_abs_error_lbs / len(test_inputs)

    print(f"æ¸¬è©¦é›†æ¨£æœ¬æ•¸: {len(test_inputs)}")
    print(f"æœ€çµ‚ MSE Loss (Z-score): {avg_test_mse:.6f}")
    print(f"å¹³å‡çµ•å°èª¤å·® (MAE): {avg_mae_lbs:.2f} lbs")

    # --- (é¸ç”¨) é¡¯ç¤ºå‰ 5 ç­†é æ¸¬çµæœçµ¦ä½ çœ‹æ„Ÿè¦º ---
    print("\n[å‰ 5 ç­†é æ¸¬æŠ½æ¨£]")
    for i in range(5):
        out = net.forward(test_inputs[i])[0]
        p_lbs = (out * w_std) + w_mean
        r_lbs = (test_expects[i][0] * w_std) + w_mean
        print(
            f"æ¨£æœ¬ {i+1}: é æ¸¬ {p_lbs:.1f} lbs | å¯¦éš› {r_lbs:.1f} lbs | èª¤å·® {abs(p_lbs - r_lbs):.1f} lbs"
        )

    print("æœ€å¾Œæ¬Šé‡:")
    net.show_weights()


def run_task2_training():
    """åŸ·è¡Œ Titanic åˆ†é¡è¨“ç·´ã€é…ç½®èˆ‡è©•ä¼°"""
    
    print("\n[Step 1] è®€å–èˆ‡é è™•ç†æ•¸æ“š...")
    raw_data = read_csv_to_dict(TITANIC_FILE)
    
    # åŸ·è¡Œç‰¹å¾µå·¥ç¨‹
    final_feature_matrix, target_labels, input_nodes = (
        preprocess_and_standardize_titanic_data(raw_data)
    )
    
    # åŸ·è¡Œæ•¸æ“šæ‹†åˆ†
    X_train, X_test, Y_train, Y_test = train_test_split_pure_python(
        final_feature_matrix, target_labels, TEST_SIZE_RATIO, RANDOM_SEED
    )

    print(f"ç‰¹å¾µæ•¸: {input_nodes}")
    print(f"è¨“ç·´é›†: {len(X_train)} ç­†, æ¸¬è©¦é›†: {len(X_test)} ç­†")

    # [Step 2] å»ºç«‹æ¨¡å‹é…ç½®ä¸¦åˆå§‹åŒ–
    print("\n[Step 2] å»ºç«‹ç¥ç¶“ç¶²è·¯æ¨¡å‹...")
    model_config = build_titanic_model_config(input_nodes)
    
    # åˆå§‹åŒ–ç¶²è·¯ (å°‡ dict è½‰ç‚º json string å‚³å…¥)
    net = Network(json.dumps(model_config)) 
    net.generate_random_weight(-0.1, 0.1) # éš¨æ©Ÿåˆå§‹åŒ–æ¬Šé‡

    # [Step 3] è¨­å®šè¨“ç·´åƒæ•¸
    # å¦‚æœä½ çš„ Network æ”¯æ´ "cross_entropy"ï¼Œé€™è£¡æ”¹ç”¨å®ƒæœƒæ›´å¥½ï¼›å¦å‰‡ä½¿ç”¨ "mse"
    loss_name = "binary_cross_entropy" 
    loss_func = Network.Loss.binary_cross_entropy 
    
    learning_rate = 0.05 
    epochs = 200        # éœ€è¦è¼ƒå¤šè¼ªæ¬¡ä¾†æ”¶æ–‚

    print(f"\n[Step 3] é–‹å§‹è¨“ç·´ (Epochs: {epochs}, LR: {learning_rate})...")
    
    for epoch in range(epochs):
        total_loss = 0
        train_preds = []

        # --- æ‰¹æ¬¡è¨“ç·´ (Stochastic Gradient Descent) ---
        for x, y_scalar in zip(X_train, Y_train):
            # y_scalar æ˜¯ int (0 æˆ– 1)ï¼Œç¥ç¶“ç¶²è·¯é€šå¸¸æœŸå¾…åˆ—è¡¨æ ¼å¼ [0] æˆ– [1]
            y_target = [y_scalar]

            # 1. Forward
            output = net.forward(x)
            train_preds.append(output)

            # 2. Backward
            net.set_output_gradients(output, y_target, loss_name) 
            net.backward()
            net.zero_grad(learning_rate)
            
            # ç´¯ç© Loss (åƒ…ä¾›è§€å¯Ÿ)
            total_loss += loss_func(y_target, output)

        # æ¯ 20 è¼ªå°å‡ºä¸€æ¬¡ç‹€æ…‹
        if epoch % 20 == 0:
            avg_loss = total_loss / len(X_train)
            acc = calculate_accuracy(train_preds, Y_train)
            print(f"Epoch {epoch}: Loss = {avg_loss:.6f} | Train Acc = {acc*100:.1f}%")

    print("\næœ€çµ‚æ¬Šé‡")
    net.show_weights()
    # [Step 4] æœ€çµ‚æ¸¬è©¦è©•ä¼°
    print("\n[Step 4] æœ€çµ‚æ¸¬è©¦é›†è©•ä¼°...")
    
    test_preds = []
    for x in X_test:
        output = net.forward(x)
        test_preds.append(output)
    
    final_acc = calculate_accuracy(test_preds, Y_test)
    print(f"========================================")
    print(f"ğŸ† Titanic æ¸¬è©¦é›†æº–ç¢ºç‡: {final_acc*100:.2f}%")
    print(f"========================================")
    
    # é¡¯ç¤ºå‰ 5 ç­†é æ¸¬è©³æƒ…
    print("\n[å‰ 5 ç­†æ¸¬è©¦æ¨£æœ¬é æ¸¬è©³æƒ…]")
    for i in range(min(5, len(X_test))):
        prob = test_preds[i][0]
        pred = "ç”Ÿé‚„" if prob >= 0.5 else "æ­»äº¡"
        actual = "ç”Ÿé‚„" if Y_test[i] == 1 else "æ­»äº¡"
        status = "âœ…" if pred == actual else "âŒ"
        print(f"æ¨£æœ¬ {i}: é æ¸¬æ©Ÿç‡ {prob:.4f} ({pred}) | å¯¦éš›: {actual} {status}")


if __name__ == "__main__":

    print("========================================")
    print("        ğŸš€ åŸ·è¡Œ Task 1: Gender-Weight (è¿´æ­¸) ğŸš€")
    print("========================================")
    run_task1_training() # åŸ·è¡Œè¿´æ­¸ä»»å‹™çš„å®Œæ•´æµç¨‹

    print("\n========================================")
    print("        ğŸš¢ åŸ·è¡Œ Task 2: Titanic (åˆ†é¡) ğŸš¢")
    print("========================================")
    # run_task2_training()
